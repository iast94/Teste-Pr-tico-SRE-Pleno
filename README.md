# Teste-Pr-tico-SRE-Pleno
## üê≥Tarefa 1: Containeriza√ß√£o & Execu√ß√£o - Decis√µes T√©cnicas: Dockerfile

A estrat√©gia de containeriza√ß√£o foi focada em seguran√ßa, otimiza√ß√£o de camadas e confiabilidade para atender aos requisitos de SRE Pleno.

### 1. Imagem Base: Node 20-alpine (Active LTS)
* **Escolha:** Foi utilizada a vers√£o `node:20-alpine`.
* **Justificativa de Tamanho:** O Alpine Linux √© uma distribui√ß√£o minimalista reduzindo assim o tempo de download (pull) e o consumo de storage no cluster.
* **Justificativa de Seguran√ßa:** Por conter apenas o essencial para a execu√ß√£o do SO, o Alpine possui menos bin√°rios e bibliotecas instaladas. Isso reduz drasticamente a "superf√≠cie de ataque", diminuindo o n√∫mero de vulnerabilidades (CVEs) potenciais que ferramentas de scan podem encontrar.

### 2. Otimiza√ß√£o de Build: Multi-Stage e Cache
* **Aproveitamento de Cache:** A c√≥pia dos arquivos `package.json` e `yarn.lock` foi realizada antes da c√≥pia do restante do c√≥digo fonte. Como o Docker funciona em camadas (layers), isso garante que, se o c√≥digo mudar mas as depend√™ncias n√£o, o Docker reutilize a camada de instala√ß√£o (cache), acelerando o tempo de build no pipeline CI/CD.
* **Multi-Stage Build:** Foi implementada a separa√ß√£o entre o est√°gio de constru√ß√£o (build) e o de execu√ß√£o (runtime). O ambiente final cont√©m apenas os artefatos compilados, eliminando compiladores e arquivos fonte, o que garante uma imagem mais leve e segura para o ambiente de stagin.

### 3. Seguran√ßa: Usu√°rio Non-Root com ID Fixo
* **Implementa√ß√£o:** Foi criado um grupo e usu√°rio espec√≠fico (`appuser`) com ID fixo `1001`.
* **Justificativa do ID 1001:** O uso de um UID/GID fixo acima de 1000 √© uma conven√ß√£o de seguran√ßa para garantir que o usu√°rio da aplica√ß√£o n√£o coincida com usu√°rios do sistema host (como o root, que √© ID 0). Al√©m disso, IDs fixos facilitam a gest√£o de permiss√µes de volumes (RBAC) e pol√≠ticas de seguran√ßa do pod (PodSecurityPolicies) no Kubernetes.
* **Privil√©gios M√≠nimos:** Rodar o processo como non-root impede que, em caso de invas√£o da aplica√ß√£o, o atacante obtenha privil√©gios administrativos sobre o kernel do n√≥ hospedeiro.

### 4. Execu√ß√£o: Bin√°rio Direto vs Gerenciadores
* **Comando:** Foi definido o uso de `CMD ["node", "dist/main.js"]`.
* **Sinais do Sistema:** O Node.js foi configurado como o processo principal (PID 1) para que possa receber sinais de termina√ß√£o do Kubernetes, como o `SIGTERM`. Gerenciadores como `npm` ou `yarn` costumam "encapsular" o processo, impedindo que os sinais cheguem ao Node, o que inviabilizaria um Graceful Shutdown (desligamento limpo).
* **Determinismo:** O uso do par√¢metro `--frozen-lockfile` no build garante que as vers√µes das depend√™ncias instaladas sejam exatamente as testadas, evitando desvios entre ambientes.

## ‚ò∏Ô∏è Tarefa 2: Deployment Kubernetes - Decis√µes T√©cnicas: Helm & Kubernetes

A arquitetura de deployment foi projetada para garantir alta disponibilidade, escalabilidade autom√°tica e isolamento de recursos, seguindo as melhores pr√°ticas de infraestrutura como c√≥digo.

### 1. Parametriza√ß√£o e Reutiliza√ß√£o (Helm)
* **Abstra√ß√£o via Values:** Todos os par√¢metros sens√≠veis e de configura√ß√£o (portas, caminhos de health check, limites de recursos) foram movidos para o arquivo `values.yaml`. Isso permite que o mesmo chart seja utilizado em diferentes ambientes apenas alterando o arquivo de valores, sem a necessidade de modificar os templates base.
* **Uso de Helpers:** Foi implementado o arquivo `_helpers.tpl` para gerenciar a nomenclatura dos recursos e labels de forma din√¢mica. O uso da fun√ß√£o `fullname` garante a unicidade dos nomes dentro do cluster, evitando colis√µes de recursos entre diferentes releases.

### 2. Alta Disponibilidade e Distribui√ß√£o (Topology Spread Constraints)
* **Estrat√©gia de Espalhamento:** Foi utilizada a funcionalidade de `topologySpreadConstraints` com `maxSkew: 1` e `topologyKey: kubernetes.io/hostname`. 
* **Justificativa:** Diferente de uma afinidade simples, o Spread Constraint garante matematicamente que as r√©plicas da aplica√ß√£o sejam distribu√≠das de forma equilibrada entre os n√≥s dispon√≠veis (`node-01` e `node-02`). O uso de `whenUnsatisfiable: DoNotSchedule` assegura que o cluster n√£o concentre pods em um √∫nico n√≥, mitigando o risco de downtime total em caso de falha de um host f√≠sico.

### 3. Resili√™ncia e Ciclo de Vida (PDB e Probes)
* **Pod Disruption Budget (PDB):** Foi implementado um PDB com `minAvailable: 1`. Esta configura√ß√£o √© vital para opera√ß√µes de SRE, pois impede que manuten√ß√µes automatizadas (como o dreno de um n√≥) desliguem todas as inst√¢ncias da aplica√ß√£o simultaneamente, garantindo que pelo menos 50% da capacidade esteja sempre ativa.
* **Health Checks Din√¢micos:** As Probes de `liveness` e `readiness` foram parametrizadas para validar a sa√∫de da aplica√ß√£o em tempo real. A separa√ß√£o entre liveness (rein√≠cio do container) e readiness (entrada no balanceador) garante que o tr√°fego s√≥ seja direcionado para pods que completaram seu processo de inicializa√ß√£o.

### 4. Escalabilidade Autom√°tica (HPA v2)
* **M√©tricas Combinadas:** O Horizontal Pod Autoscaler foi configurado para monitorar tanto CPU quanto Mem√≥ria simultaneamente.
* **Thresholds de Performance:** Foram definidos gatilhos de **70% para CPU** e **75% para Mem√≥ria**, conforme requisitos t√©cnicos do projeto. Esta abordagem h√≠brida protege a aplica√ß√£o contra gargalos de processamento e vazamentos de mem√≥ria (memory leaks), garantindo que o cluster escale horizontalmente de forma proativa antes da degrada√ß√£o da lat√™ncia.

### 5. Estrat√©gia de Deploy (Rolling Update)
* **Zero Downtime:** Foi configurada a estrat√©gia `RollingUpdate` com `maxUnavailable: 0`. Isso garante que o Kubernetes nunca remova uma vers√£o antiga da aplica√ß√£o sem antes ter uma nova vers√£o saud√°vel e pronta para receber tr√°fego, eliminando quedas de servi√ßo durante atualiza√ß√µes de vers√£o.